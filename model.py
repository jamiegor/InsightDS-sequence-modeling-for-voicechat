import numpy as np
import pandas as pd
import warnings
import matplotlib.pyplot as plt

from keras.models import Sequential
from keras.metrics import binary_accuracy
from keras.losses import binary_crossentropy
from keras.preprocessing.sequence import pad_sequences
from keras.optimizers import Adam
from keras import layers

from sklearn.model_selection import cross_val_predict, cross_val_score
from sklearn.metrics import precision_score, recall_score, accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.base import TransformerMixin


def load_cleaned_data(call_path, order_path):
    categoricals = ['GenderCd', 'HubId', 'BrandGLCode', 'dimMemLoyaltyStatusId']
    calls  = pd.read_csv(call_path, parse_dates=['IVRCallBeginDtm'],
                         dtype={col:"category" for col in categoricals})

    orders = pd.read_csv(order_path, parse_dates=['OrderDtm'])
    return calls, orders


class MakeHistories(TransformerMixin):

    """
    Version of construct_histories suitable for sklearn pipepline

    Transform method:
    INPUT
    A list [calls, orders], where
      calls:   dataframe containing all calls
      orders:  dataframe containing all orders

    OUTPUTS
    call_hist:   df of calls indexed by swPersonId, and ordered chronologically
                 within each swP
    order_hist:  df of orders indexed by swPersonId, and ordered chronologically
                 within each swP
    """

    def __init__(self,
                 genders='M'):
        self.genders = genders

    def fit(self, X, y=None):
        return self
    def transform(self, X, y=None):
        call_hist, order_hist = construct_histories(
            X[0],
            X[1])
        return call_hist, order_hist


class SampleAndLabel(TransformerMixin):

    """
    The transform method: For each date in a list, it truncates call_hist,
    order_hist at that date and computes the corresponding 'churn' labels by
    looking at the subsequent (future) window.
    People are also filtered out according to the timing of their last calls &
    orders, and the total number of calls/orders placed before sampling date.


    INPUT (for transform method)
    call_hist, order_hist: df's generated by 'construct_histories'

    KWARGS
    dates:        list of dates at which to sample
    churn_metric: what to define churn in terms of (options: 'calls' , 'orders')
    churn_days:   size of future window to examine in determining churn
                  counted starting from last call/order (not from 'date')
                  (after this period of inactivity customer is considered lost)
    # drop_censored: None, or 'datetime' when data collection terminated. Drops
    #                people who called (ordered) within churn_days of this enddate

    OUTPUT
    labels:     Series of labels, indexed by swPersonId.
                swPersonId's are mult. by 10 for each subsequent sample,
                in order to distinguish them
    call_trunc:
    order_trunc:
    cohort:     list of unique swPersonId's contained in the multi-date sample
    sample_sizes:
    dates:      same as input
    """
    def __init__(self,
                 dates=['2016-05-01 04:00:00'],
                 churn_metric='calls',
                 churn_days=90,
                 min_calls=10,
                 min_paidorders=1,
                 lastorder_window=(90,0),
                 lastcall_window=(90,0),
                 allow_person_resample=True,
                 sample_end=None,
                 events_remaining=4
                 ):
        assert type(dates) == list
        self.dates = [pd.to_datetime(date) for date in dates]
        self.churn_metric = churn_metric
        self.churn_days = churn_days
        self.min_calls = min_calls
        self.min_paidorders= min_paidorders
        self.lastcall_window = lastcall_window
        self.lastorder_window = lastorder_window
        self.allow_person_resample = allow_person_resample
        self.sample_end = sample_end
        self.events_remaining = events_remaining

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        call_hist, order_hist = X[0], X[1]

        assert self.churn_metric in ['calls', 'orders']
        j = ['calls', 'orders'].index(self.churn_metric)
        hist = X[j]

        labels = pd.Series()
        cohort = pd.Index([])
        call_trunc, order_trunc = pd.DataFrame(), pd.DataFrame()
        sample_sizes = []

        for n, date in enumerate(self.dates):

            # Check for 'censoring': sample cutoff shouldn't be close to enddate
            if self.sample_end is not None:
                if date + pd.Timedelta(self.churn_days, 'D') > self.sample_end:
                    warnings.warn('Sample {} is censored by end date '.format(n)
                                  +'(not enough time to determine churn label)')

            call_trunc_n, order_trunc_n = (
                truncate_date(call_hist,
                              order_hist,
                              date=date)  )

            order_trunc_n['sample_date'] = date
            call_trunc_n['sample_date'] = date

            hist_trunc = [call_trunc_n, order_trunc_n][j]

            #########  FILTERING  #########
            people = filterby(order_trunc_n.query("OrderStatus=='Paid'"),
                         by='orders',
                         date=date,
                         min_num=self.min_paidorders,
                         past_window=self.lastorder_window )
            people = people.intersection(
                filterby(call_trunc_n,
                         by='calls',
                         date=date,
                         min_num=self.min_calls,
                         past_window=self.lastcall_window) )

            if not self.allow_person_resample:
                people = people.difference(cohort)

            order_trunc_n = order_trunc_n.loc[people]
            call_trunc_n = call_trunc_n.loc[people]

            #########  Compute labels  #########
            labels_n = (
                churn_labels(hist.loc[people],
                             hist_trunc.loc[people],
                             by=self.churn_metric,
                             date=date,
                             events_remaining=self.events_remaining,
                             churn_days=self.churn_days ) )

            # cohort keeps track of unique individuals across date-samples
            cohort = cohort.union(labels_n.index)
            sample_sizes.append(len(labels_n))

            # indexes distinguish re-occurrences of same swPersonId
            # in later samples using powers of 10
            for obj in [labels_n, order_trunc_n, call_trunc_n]:
                obj.index = obj.index * 10**n

            labels = pd.concat([labels, labels_n])
            order_trunc = pd.concat([order_trunc, order_trunc_n ])
            call_trunc  = pd.concat([call_trunc, call_trunc_n ])

        return labels, call_trunc, order_trunc, cohort, sample_sizes, self.dates



class MakeSequenceFeatures(TransformerMixin):
    """
    MakeFeatures.transform method:
    Takes truncated call/order histories, list of truncation (sampling) dates,
    and outputs a df of features, indexed by swPersonId

    KWARGS

    INPUT (transform method)
    labels, call_trunc, order_trunc, cohort, sample_sizes, dates

      call_hist:   df of calls indexed by swPersonId, and ordered
                   chronologically within each swP
      order_hist:  df of orders indexed by swPersonId, and ordered
                   chronologically within each swP

    OUTPUT (transform method)
    labels, combined, cohort, dates
    """

    def __init__(self,
                 Loyalty=False,
                 OrderStatus=True,
                 Brand=False,
                 Hub=False,
                 call_hr=False,
                 call_day=False):
        self.Loyalty = Loyalty
        self.OrderStatus = OrderStatus
        self.Brand = Brand
        self.Hub = Hub
        self.call_hr = call_hr
        self.call_day = call_day

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        labels, call_trunc, order_trunc, cohort, sample_sizes, dates = X

        call_trunc['rectype'] = 1
        order_trunc['rectype'] = 0

        order_trunc = (order_trunc.query("OrderStatus in ['Paid', 'Cancelled']")
                       .copy() )

        call_fields = ['rectype', 'IVRCallBeginDtm', 'IVRCallDurationSec',
                       'InteractionsReceived', 'DistinctANISenderCnt',
                       'InteractionsInitiated', 'DistinctANITargetCnt',
                       'sample_date','dimMemLoyaltyStatusId']
        order_fields = ['OrderDtm', 'sample_date', 'OrderStatus',
                         'TotalPrice',  'rectype', 'BrandGLCode']
        call_trunc = call_trunc.loc[:, call_fields]
        order_trunc = order_trunc.loc[:, order_fields]

        order_cats = ['rectype']
        call_cats = ['rectype']

        timestamps = [(call_trunc, 'IVRCallBeginDtm', call_cats),
                      (order_trunc, 'OrderDtm', order_cats)]
        if self.call_day:
            for df, col, cat in timestamps:
                df[ ['day'+str(i) for i in range(7)] ] = (
                    pd.get_dummies(df[col].copy().dt.dayofweek) )
                cat += ['day'+str(i) for i in range(7)]
        if self.call_hr:
            for df, col, cat in timestamps:
                df[ ['hr'+str(i) for i in range(12)]] = (
                    pd.get_dummies(df[col].copy().dt.hour//2 ) )
                cat += ['hr'+str(i) for i in range(12)]

        # Add LoyaltyStatus, with 1hot encoding:
        if self.Loyalty:
            call_trunc[['loyal'+str(i+1) for i in range(8)]] = pd.get_dummies(
                call_trunc.dimMemLoyaltyStatusId )
            call_cats += ['loyal'+str(i+1) for i in range(8)]
        call_trunc.drop('dimMemLoyaltyStatusId', axis=1, inplace=True)

        ## Optional: Include HubId with 1hot
        if self.Hub:
            assert call_trunc.HubId.nunique() == 14
            call_trunc[['Hub'+str(i) for i in range(14)]] = (
                pd.get_dummies(call_trunc.HubId).drop(columns=['-1']))

        # Transform timestamps to give time between 'date' and event:
        # this must come after 1hot enc of day/hr
        for df, col, _ in timestamps:
            df[col] = (df[col] - df['sample_date']) / pd.Timedelta(1,'D')
            df.drop('sample_date', axis=1, inplace=True)
            df = df.rename(columns={col:'dtm'})
            # break

        # 1hot encode OrderStatus & Brand of purchase
        if self.OrderStatus:
            order_trunc[['Paid', 'Cancelled']] = pd.get_dummies(
                                                    order_trunc.OrderStatus)
            order_cats += ['Paid', 'Cancelled']
        order_trunc.drop('OrderStatus', axis=1, inplace=True)

        if self.Brand:
            order_trunc[ ['brand'+str(int(br))
                          for br in order_trunc.BrandGLCode.unique()]
                        ] = pd.get_dummies( order_trunc.BrandGLCode)
            order_cats += [
                'brand'+str(int(br)) for br in order_trunc.BrandGLCode.unique()]

        order_trunc.drop('BrandGLCode', axis=1, inplace=True)
        call_trunc.rename(columns={'IVRCallBeginDtm':'dtm'}, inplace=True)
        order_trunc.rename(columns={'OrderDtm':'dtm'}, inplace=True)

        # Sqrt-transform and normalize data
        for df, cats in [(call_trunc, call_cats), (order_trunc, order_cats)]:
            scaler = StandardScaler()
            numer_cols = list( set(df.columns) - set(cats) )
            non_dtm= [col for col in numer_cols if col != 'dtm']
            df[non_dtm ] = np.sqrt(df[non_dtm])
            df[numer_cols] = scaler.fit_transform(df[numer_cols])

        combined = pd.concat([call_trunc, order_trunc]).fillna(0)
        combined = combined.sort_values(by=['swPersonId','dtm'], axis=0)

        #  Since masking/padding uses zeroes, change categorical/one-hot
        #  columns to use positive integers
        combined.loc[:, call_cats + order_cats] += 1

        # Some CallDurations are negative: take abs
        combined.IVRCallDurationSec = combined.IVRCallDurationSec.abs()

        return labels, combined, cohort, dates

class ShuffleAndSplit(TransformerMixin):

    """
    Randomly chooses a fraction test_frac of people, then puts all their
    samples in test_set after shuffling. Remainder are shuffled and put
    in train_set.
    """

    def __init__(self,
                 test_frac=0.1,
                 return_ids=False):
        self.test_frac = test_frac
        self.return_ids= return_ids

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):

        labels, combined, cohort, dates = X

        # Randomly sample test_frac of the *unique real* people in data set:
        # np.random.seed(4)
        ppl = np.random.permutation(cohort.values)
        test_ppl = ppl[ :round( self.test_frac * len(ppl)) ]
        train_ppl= ppl[ round( self.test_frac * len(ppl)): ]

        # Each real person appears in some subset of the len(date) samples.
        # n'th sample has swPersonID (index) values multiplied by 10**n
        # Take test people, get all their possible swPersonID's, and compute the
        # intersection with actual swPersonId's occurring in dataset, i.e. 'combined'
        test_indices = np.random.permutation( list(
            set(pd.concat( [pd.Series(test_ppl) * 10**n
                            for n in range(len(dates))]))
            & set(combined.index) ) )

        train_indices = np.random.permutation( list(
            set(pd.concat( [pd.Series(train_ppl) * 10**n
                            for n in range(len(dates))]))
            & set(combined.index)))

        test_set = combined.loc[test_indices]
        train_set = combined.loc[train_indices]

        test_labels = labels.loc[test_indices]
        train_labels = labels.loc[train_indices]

        if self.return_ids:
            return (train_indices, test_indices, train_set, test_set,
                    train_labels, test_labels)
        else:
            return train_set, test_set, train_labels, test_labels

class MakePaddedSequences(TransformerMixin):

    """
    Converts each of train and test sets into list of lists.
    The inner lists are our sequences, truncated/padded as appropriate to
    have fixed, specified length
    """

    def __init__(self,
                 maxlen=150,
                 return_ids=False):
        self.maxlen = maxlen
        self.return_ids = return_ids

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        if self.return_ids:
            (train_indices, test_indices, train_set, test_set,
             train_labels, test_labels) = X
        else:
            train_set, test_set, train_labels, test_labels = X

        # list of lists of feature vectors
        train_sequences = [
            [row.values for index, row in train_set.loc[person].iterrows()]
            for person in train_set.index.unique()]

        test_sequences = [
            [row.values for index, row in test_set.loc[person].iterrows()]
            for person in test_set.index.unique()]

        pad_trainseqs = pad_sequences(
            train_sequences, maxlen=self.maxlen, dtype='float32',
            padding='pre', truncating='pre', value=0.0)
        pad_testseqs = pad_sequences(
            test_sequences, maxlen=self.maxlen, dtype='float32',
            padding='pre', truncating='pre', value=0.0)

        if self.return_ids:
            return (train_indices, test_indices,
             pad_trainseqs, pad_testseqs, train_labels, test_labels)
        else:
            return pad_trainseqs, pad_testseqs, train_labels, test_labels


class MakeFlatFeatures(TransformerMixin):
    """
    Constructs a flat list of features for each swPersonId. For use by
    non-sequence models.

    MakeFlatFeatures.transform method:
      Takes truncated call/order histories & list of truncation (sampling)
      dates, and outputs a df of features, indexed by *unique* swPersonId.
      (Cf. MakeSequenceFeatures, whose output (df called 'combined') contains
      a row for each event, ie. many rows per swPersonId)

    KWARGS

    INPUT (transform method)
    labels, call_trunc, order_trunc, cohort, sample_sizes, dates

      call_hist:   df of calls indexed by swPersonId, and ordered
                   chronologically within each swP
      order_hist:  df of orders indexed by swPersonId, and ordered
                   chronologically within each swP

    OUTPUT (transform method)
    labels, flatfeatures, cohort, dates
    """

    def __init__(self,
                 brand_names={'1': 'LLK', '2': 'IML', '3': 'TGO',
                                    '4': 'RDL', '7': 'VBL', '8': 'FON',
                                    '12':'LBL','52':'GSV'},
                 use_cat=False,
                 take_sqrt=True):
        self.brand_names = brand_names
        self.use_cat = use_cat
        self.take_sqrt = take_sqrt

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        labels, call_trunc, order_trunc, cohort, sample_sizes, dates = X
        # Some CallDurations are negative: take abs
        call_trunc.IVRCallDurationSec = call_trunc.IVRCallDurationSec.abs()

        features = pd.DataFrame(index=call_trunc.index.unique())

        grouped = call_trunc.groupby(level=0)

        #lifespan in days (fractional)
        features['lifespan'] = ( grouped['IVRCallBeginDtm'].last() -
                               grouped['IVRCallBeginDtm'].first()
                               ) / pd.Timedelta(1, 'D')

        features['calltime'] = grouped['IVRCallDurationSec'].sum() /60.
        features['callcount'] = grouped['IVRCallDurationSec'].count()
        features['frequency'] = features['lifespan'] / features['callcount']

        #intrxns
        intrxns = ['InteractionsReceived', 'DistinctANISenderCnt', \
                   'InteractionsInitiated', 'DistinctANITargetCnt']
        abbrevs = list(zip(intrxns, ['IR', 'DASC', 'II', 'DATC']))
        features = pd.concat(
            [features,
             grouped[intrxns].mean().rename(
                 columns={abbrev[0]:abbrev[1]+'_mean' for abbrev in abbrevs}),
             grouped[intrxns].max().rename(
                  columns={abbrev[0]:abbrev[1]+'_max' for abbrev in abbrevs}),
             grouped[intrxns].median().rename(
                  columns={abbrev[0]:abbrev[1]+'_median' for abbrev in abbrevs})
             ], axis=1)

        gaps = grouped['IVRCallBeginDtm'].diff()/pd.Timedelta(1, 'D')
        gaps = gaps.dropna().groupby('swPersonId') # (diffs all start with NaN)
        for attr in ['mean', 'std', 'max', 'min', 'median']:
            #add columns for "gap" statistics and fill NaN's with zeroes:
            features = features.join(
                getattr(gaps, attr)().rename('gaps_'+attr),
                how='outer'
                ).fillna(0)

        current_gap = []
        ind = 0
        for i, dt in enumerate(dates):
            current_gap = current_gap + list(
                ((pd.to_datetime(dt) - (call_trunc.groupby('swPersonId')
                                        .IVRCallBeginDtm
                                        .last()[ind:sum(sample_sizes[:i+1])] )
                  ) / pd.Timedelta(1,'D') ).values )
            ind += sample_sizes[i]

        features['current_gap'] = current_gap

        # Take square root of all numeric cols except 'lifespan':
        if self.take_sqrt:
            sqrt_cols = list( set(features.columns) - set(['lifespan']) )
            features[sqrt_cols ] = np.sqrt(features[sqrt_cols])

        # Normalize data
        scaler = StandardScaler()
        features.loc[:,:] = scaler.fit_transform(features)

        # Add categorical features for brands & hubs:
        if self.use_cat:
            # BRANDS (one-hot)
            brands = grouped['BrandGLCode'].agg(lambda x: x.mode()[0])
            features = pd.concat([features,
                                  pd.get_dummies(brands).rename( columns={
                br:'brand'+br for br in call_trunc.BrandGLCode.unique() }
                                                                )
                                  ], axis=1)

            # HUBS (one-hot-encoded)
            hubs = grouped['HubId'].agg(lambda x: x.mode()[0])
            features = pd.concat([features,
                                  pd.get_dummies(hubs).rename( columns={
                hub:'Hub'+hub for hub in call_trunc.HubId.unique() }
                                                              )
                                  ], axis=1)

        return labels, features, cohort, dates


def RNN(train_set, train_labels,
        val_data=None,
        print_summary=True,
        plot_results=True,
        rnn_type='LSTM',
        dense_layers=1,
        rnn_layers=1,
        dense_size=16,
        rnn_size=32,
        epochs=10,
        lr=0.005,
        decay=0.0,
        checkpoint=False,
        filename='test_rnn.h5',
        earlystopping=False,
        batch_size=64,
        # val_split=0.15,
        dropout=0.3,
        rec_dropout=0.3,
        verbose=2):

    assert rnn_type in ['GRU', 'LSTM']
    optim = Adam(lr=lr,
                 decay=decay)
    loss = 'binary_crossentropy'
    callbacks=None
    if checkpoint or earlystopping:
        callbacks=[]
        if checkpoint:
            from keras.callbacks import ModelCheckpoint
            callbacks.append(ModelCheckpoint(
                filename, monitor='val_loss', save_best_only=True))
        if earlystopping:
            from keras.callbacks import EarlyStopping
            callbacks.append(EarlyStopping(monitor='val_loss', patience=10 ))
    # callbacks = [ EarlyStopping(monitor='val_loss', patience=2 ) ]
    num_features = train_set.shape[-1]
    max_len = train_set.shape[1]
    input_shape = (max_len, num_features)
    # steps_per_epoch = train_set.shape[0]//batch_size

    model = Sequential()

    model.add(
        layers.Masking(mask_value=0., input_shape=(max_len, num_features)))

    for _ in range(dense_layers):
        model.add(
            layers.TimeDistributed(layers.Dense(dense_size, activation='tanh')))

    for layer in range(rnn_layers):
        if rnn_type == 'GRU':
            model.add(
                layers.GRU(rnn_size, dropout=dropout,
                           recurrent_dropout=rec_dropout,
                           input_shape=(max_len, num_features),
                           return_sequences= (not (layer==(rnn_layers-1)))
                            ) )
        if rnn_type == 'LSTM':
            model.add(
                layers.LSTM(
                    rnn_size, dropout=dropout, recurrent_dropout=rec_dropout,
                    input_shape=(max_len, num_features),
                    return_sequences= (not (layer==(rnn_layers-1)))
                     ) )

    model.add(layers.Dense(1, activation='sigmoid'))

    if verbose > 0: print('Compiling model...')
    model.compile(loss=loss, optimizer=optim, metrics=['binary_accuracy'])

    if verbose > 0: print('Training model...')
    history = model.fit(x=train_set, y=(1*train_labels).values,
                        batch_size=batch_size,
                        epochs=epochs,
                        shuffle=True,
                        callbacks=callbacks,
                        validation_data=val_data,
                        verbose=verbose
                       )

    if plot_results:
        losshist = history.history['loss']
        val_loss = history.history['val_loss']
        acc = history.history['val_binary_accuracy']
        epochs = range(1, len(losshist) + 1)
        plt.figure();
        plt.plot(epochs, losshist, 'o', label='Training loss');
        plt.plot(epochs, val_loss, '-', label='Validation loss');
        plt.plot(epochs, acc, '-', label='Binary Accuracy');
        plt.title('Losses & accuracy');
        plt.legend();
        plt.show();

    if print_summary:
        test_set = val_data[0]
        test_labels = val_data[1]
        test_ids = test_labels.index
        predictions = pd.Series(model.predict_classes(test_set)[:,0],
                 index=test_ids)
        print('Confusion matrix: \n')
        print(confusion_matrix(test_labels, predictions))
        target_ids = predictions.loc[predictions>0].index
        print('accuracy: ',
              round(100*accuracy_score(test_labels, predictions)), '%')
        print('precision: ',
              round(100*precision_score(test_labels, predictions)), '%',
              'recall: ',
              round(100*recall_score(test_labels, predictions)) , '%')
        print("churn fraction of training set: ",
              train_labels.sum()/len(train_labels) )
        print("Fraction of test set predicted to churn: ",
              len(target_ids)/len(test_set))

    return model, history



def construct_histories(calls,
                        orders,
                        genders='M'):
    """
    Construct dataframes of calls & orders, indexed by swPersonId,
    and ordered by date within each swPersonId.

    INPUT
    calls:      df of calls containing a column 'swPersonId'
    genders:    list (or just str if singleton) of gender codes

    OUTPUT
    call_hist:  ordered df of calls, indexed by swPersonId
    order_hist: ordered df of orders, indexed by swPersonId
    """

    genders = list(set(genders))
    calls_g = calls.loc[ calls['GenderCd'].isin(genders) ]

    call_hist = (calls_g.sort_values(
        by=['swPersonId', 'IVRCallBeginDtm'], axis=0)
                 .set_index('swPersonId') )

    persons = call_hist.index.unique().values

    order_hist = (orders.sort_values(
        by=['swPersonId', 'OrderDtm'], axis=0)
                  .set_index('swPersonId') )
    # only keep people that appear in call_hist:
    order_hist = order_hist.loc[ order_hist.index.isin(call_hist.index) ]
    order_hist.dropna(
        subset=['ANI', 'PackagePrice', 'TotalPrice'], how='any', inplace=True)

    return call_hist, order_hist

def truncate_date(call_hist,
                  order_hist,
                  date='2018-09-20 04:00:00'):
    """
    Truncate all call/order histories at specified date
    Returns new df's containing calls and orders before 'date'
    """
    date = pd.to_datetime(date)
    call_trunc = call_hist.loc[call_hist.IVRCallBeginDtm < date].copy()
    order_trunc= order_hist.loc[order_hist.OrderDtm < date].copy()
    order_trunc = order_trunc.loc[ order_trunc.index.isin(call_trunc.index) ]

    return call_trunc, order_trunc

def churn_labels(hist,
                 hist_trunc,
                 by='calls',
                 date='2018-09-21 04:00:00',
                 events_remaining=0,
                 churn_days=90
                 ):
    """
    Computes labels using either call_hist or order_hist

    NOTE: People (swPersonId) with no paid orders before 'date' will be dropped
     -> filter out such people before passing to churn_labels

    INPUT
    hist:       either call_hist or order_hist
    hist_trunc: hist, truncated after 'date'
    by:         'calls' or 'orders'
    date:       date of truncation
    churn_days: size of future window to examine for churn label
    events_remaining: how many calls/orders (cf 'by' kwarg) can still happen
                      within churn_days for person to count as 'churned'

    OUTPUT
    labels:      Series of True/False labels, indexed by unique swPersonId
                 ('True' denotes a 'churner' )

    """

    if type(date)==str:
        date = pd.to_datetime(date)
    if by=='orders':
        paidorders_to_date = (hist_trunc
                               .loc[hist_trunc.OrderStatus=='Paid']
                               .groupby(level=0)['ANI']
                               .size() )
        last_paidorder = (hist_trunc.query('OrderStatus=="Paid"')
                          # .query('OrderDtm < @date')
                          .groupby('swPersonId').OrderDtm
                          .last() )
        # date after which each person will be considered to have churned:
        cutoff = (
            pd.Series(data=hist.index, index=hist.index)
            .map(last_paidorder.to_dict()  )
            + pd.Timedelta(churn_days, 'D')
            )

        paidorders_total = (hist.loc[hist.OrderDtm < cutoff]
                             .query("OrderStatus=='Paid'")
                             .groupby(level=0).ANI
                             .size() )

        future_events = (paidorders_total
                              .loc[ paidorders_to_date.index ]
                              - paidorders_to_date)
    elif by=='calls':
        calls_to_date = (hist_trunc.groupby(level=0)['ANI'].size() )
        last_call = (hist_trunc #.query('IVRCallBeginDtm < @date')
                     .groupby('swPersonId').IVRCallBeginDtm
                     .last() )
        # date after which each person will be considered to have churned:
        cutoff = (
            pd.Series(data=hist.index, index = hist.index)
            .map(last_call.to_dict()  )
            + pd.Timedelta(churn_days, 'D')
            )

        calls_total = (hist.loc[hist.IVRCallBeginDtm < cutoff]
                       .groupby(level=0).ANI
                       .size() )

        future_events = (calls_total
                         .loc[ calls_to_date.index ]
                         - calls_to_date)

    else:
        raise Exception('"{}" is not a valid kwarg for "by"')

    labels = (future_events <= events_remaining)

    return labels


def filterby(hist_trunc,
             by='orders',
             date='2018-09-21 04:00:00',
             min_num=1,
             past_window=(90,5) ):
    """
    Identify people whose last call or order occurred within specified window
    INPUT
    hist_trunc:     order_trunc or call_trunc
    by:             'calls' or 'orders'
    date:           sampling date (at which call_/order_hist truncated)
    min_num:        min number of calls/orders before 'date'
    past_window:    tuple of two elements - max (resp. min) number of days
                    since last event (of type 'by')

    OUTPUT:         index of swPersonId's
    """
    date = pd.to_datetime(date)
    dtm = {'calls':'IVRCallBeginDtm', 'orders':'OrderDtm'}
    # assert min_num >= 1
    last_event = (hist_trunc.groupby('swPersonId')[ dtm[by] ]
                  .last())
    num_events = (hist_trunc.groupby('swPersonId')[ dtm[by] ]
                  .size())

    days_since = [pd.Timedelta(d,'D') for d in past_window]
    window_open, window_close = [date-days_since[0], date-days_since[1] ]

    people = (last_event
              .loc[ (last_event > window_open)
                     & (last_event < window_close)
                     & (num_events >= min_num) ]
              .index )

    return people


def filterby_paidorders(
                   order_hist,
                   date = '2018-09-21 04:00:00',
                   days_since_order=(5,90)
                  ):
    last_paidorder = ( order_hist.query('OrderStatus=="Paid"')
                      .query('OrderDtm < @date')
                      .groupby('swPersonId').OrderDtm
                      .last() )
    days_since_order = [pd.Timedelta(d,'D') for d in days_since_order]
    window = (date - days_since_order[0], date - days_since_order[1])

    # Last call must be within specified window (days_since_order kwarg):
    last_paidorder_filtered = (last_paidorder
                               .loc[(last_paidorder < window[0])
                                    & (last_paidorder > window[1])]
                               )

    return last_paidorder_filtered

def filterby_calls(call_hist,
                   date = '2018-09-21 04:00:00',
                   days_since_call=(0,90),
                   min_calls=1
                  ):
    """
    Identify people whose last call was within window
    INPUT: order_hist, date, days_since_order
    OUTPUT: Series with index=swPersonId, data= last paid order before date
    """
    date = pd.to_datetime(date)

    calls_grouped_trunc = ( call_hist
                           .query('IVRCallBeginDtm < @date')
                           .groupby('swPersonId') )
    last_call = calls_grouped_trunc.IVRCallBeginDtm.last()
    days_since_call = [pd.Timedelta(d,'D') for d in days_since_call]
    window = (date - days_since_call[0], date - days_since_call[1])

    num_calls = calls_grouped_trunc.IVRCallBeginDtm.size()

    # Last call must be within specified window (days_since_order kwarg):
    last_call_filtered = (last_call
                          .loc[(last_call < window[0])
                               & (last_call > window[1])
                               & (num_calls >= min_calls)]
                          )

    return last_call_filtered


def theformula(call_hist, today):
    """
    The formula as currently used by the company


    Filtering:
    Takes df of calls, drops all calls after today,
    retains only people who called in last 55 days,
    retains only their 42 most recent calls


    INPUT
    call_hist: dataframe of calls, indexed by swPersonId
           and TIME-ORDERED for each person
    OUTPUT
    index of targets
    """

    today = pd.to_datetime('2016-04-01 04:00:00')
    call_hist = pd.DataFrame(
        call_hist.query('IVRCallBeginDtm < @today').IVRCallBeginDtm )
    current_gaps = (today
                    - call_hist.groupby('swPersonId').IVRCallBeginDtm.last()
                    ) / pd.Timedelta(1,'D')

    # Expire customers who have not called in 55 days
    if current_gaps.max()>55:
        warnings.warn('Dropping some callers who have not called in 55 days')
    call_hist = call_hist.loc[
        current_gaps.where(current_gaps < 55).dropna().index ]
    current_gaps = current_gaps.loc[current_gaps < 55]

    # Keep only 42 most recent calls
    call_hist = call_hist.groupby('swPersonId').tail(42) #this is slow!
    if call_hist.groupby('swPersonId').IVRCallBeginDtm.count().min() < 2:
        s = 'Some callers have not called more than once, and are being dropped'
        warnings.warn(s)

    gaps = pd.DataFrame(
        (call_hist.groupby('swPersonId').IVRCallBeginDtm
         .diff()/pd.Timedelta(1, 'D') ).dropna()   # (diffs all start with NaN)
        )
    ave_gaps = (gaps.groupby('swPersonId').IVRCallBeginDtm
                .mean() )
    devs = (gaps.IVRCallBeginDtm
            - pd.Series(
                gaps.index.map(ave_gaps.to_dict()).values, index=gaps.index) )

    mask = (current_gaps > 2*ave_gaps)
    mod_devs =  mask * ave_gaps + (1-mask) * devs
    mod_std_devs = np.sqrt(
        (mod_devs**2).groupby('swPersonId').sum() / len(mod_devs) )

    mask2 = (call_hist.loc[mask.index]
             .groupby('swPersonId').IVRCallBeginDtm.count() >9 )

    next_call = ( call_hist.groupby('swPersonId').IVRCallBeginDtm.last()
                 + (mask2 * np.ceil(ave_gaps + 2*mod_std_devs))
                    * pd.Timedelta(1,'D')
                 + ((1-mask2) * pd.Timedelta(7,'D'))
                 )
    churn_labels = ( next_call < today)
    return churn_labels
